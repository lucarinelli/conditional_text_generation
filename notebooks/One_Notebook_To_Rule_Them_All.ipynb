{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One_Notebook_To_Rule_Them_All.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ac82f268e904a999854178e4c59fb2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5156f4c310e74a3798e2ec9c14f1f3f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ba84f66dd8841269707b1a01af199c2",
              "IPY_MODEL_dc86b11a50684a8aa7ee651cf9705731"
            ]
          }
        },
        "5156f4c310e74a3798e2ec9c14f1f3f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ba84f66dd8841269707b1a01af199c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f05aea6e898e4ab9991f1fef205ccc38",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6db73283afa9422ab02de5995547c1ce"
          }
        },
        "dc86b11a50684a8aa7ee651cf9705731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4124e284cde4a18a8ca8af0593e9d94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1174/? [00:04&lt;00:00, 332.08 tables/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2507ec0b10e42f5acb297844b661330"
          }
        },
        "f05aea6e898e4ab9991f1fef205ccc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6db73283afa9422ab02de5995547c1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4124e284cde4a18a8ca8af0593e9d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2507ec0b10e42f5acb297844b661330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3483300124ca4ca6b9bcedea43fdad04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd65a98b298f4f1f95df35c993a57b5e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a1db641052984f4681b2c805d38ff027",
              "IPY_MODEL_f8109a8db5a94027b1ba4129148106f9"
            ]
          }
        },
        "cd65a98b298f4f1f95df35c993a57b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1db641052984f4681b2c805d38ff027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e41b4f3561c64136a0a014ebfe042445",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a43412adf447443f808c9fb7645a0bce"
          }
        },
        "f8109a8db5a94027b1ba4129148106f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ca9381c022a40dd904ba9d673cd5fa4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/? [00:00&lt;00:00, 197.45 tables/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feb3df4c3fba4c16bdfc92b14616fbf6"
          }
        },
        "e41b4f3561c64136a0a014ebfe042445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a43412adf447443f808c9fb7645a0bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ca9381c022a40dd904ba9d673cd5fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feb3df4c3fba4c16bdfc92b14616fbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c591a13ea904480e93e1a3ed89962653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d10fc94ed2234b9a8fb4dec3f77d1bc1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7648849dfcd041a39fc1fbdccaa912aa",
              "IPY_MODEL_2de2324177f8494d85bcf2aed59223f2"
            ]
          }
        },
        "d10fc94ed2234b9a8fb4dec3f77d1bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7648849dfcd041a39fc1fbdccaa912aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c2112e489f684fa9a1931531b4b06aea",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e00e4b93b75040f5b952ac8d4a819431"
          }
        },
        "2de2324177f8494d85bcf2aed59223f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65393a1757db4f2498513a38386a8e35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:04&lt;00:00,  2.38s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95e2d7312d264e78a0b3b0011ba92e7c"
          }
        },
        "c2112e489f684fa9a1931531b4b06aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e00e4b93b75040f5b952ac8d4a819431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65393a1757db4f2498513a38386a8e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95e2d7312d264e78a0b3b0011ba92e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbd0f88ccd79446fa06d04b2a6586eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41340038de34465f93b28d53b2e5a7ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_80934554cef74c3cbab3b0393dc351cf",
              "IPY_MODEL_71a81f726adc46328fb348ca3a86b555"
            ]
          }
        },
        "41340038de34465f93b28d53b2e5a7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80934554cef74c3cbab3b0393dc351cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03a6c292555940839096fbb27d8dc2e3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f0ae8a8f77e4ae78cd9ee3b0b196ce4"
          }
        },
        "71a81f726adc46328fb348ca3a86b555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c74c13676d694bc18a4230079c0ae9ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:04&lt;00:00,  4.42s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_202d415297c1414ab1ef65e44e550caf"
          }
        },
        "03a6c292555940839096fbb27d8dc2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f0ae8a8f77e4ae78cd9ee3b0b196ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c74c13676d694bc18a4230079c0ae9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "202d415297c1414ab1ef65e44e550caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarinelli/conditional_text_generation/blob/main/notebooks/One_Notebook_To_Rule_Them_All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEx06PvebUnT"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r-HBtLQd2U9"
      },
      "source": [
        "## Check allocated GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0ZFIlpZd1Xk",
        "outputId": "1dcf71da-46b9-4a1e-e86a-1c618aba8fe6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 26 21:33:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    33W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi6YIukebcAm"
      },
      "source": [
        "## Install needed python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRZEIV-zWmi5"
      },
      "source": [
        "!pip install --quiet transformers datasets tokenizers sacrebleu wandb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86aJ9wg9AhWx"
      },
      "source": [
        "## Connect to WandB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0wjibP8Ak_p",
        "outputId": "197359d5-7698-41a2-9f65-4fd638500549"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucarinelli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1mJeT-fiur"
      },
      "source": [
        "## Set experiment parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s3Ve3l58mFX",
        "outputId": "e6b239b3-a710-4b22-af99-6eb272c100bb"
      },
      "source": [
        "%env WANDB_PROJECT=ctrl_dry_runs\n",
        "%env WANDB_ENTITY=polito_aiml2021_textgen\n",
        "\n",
        "experiment_parameters = dict(\n",
        "    run_name = \"exp1\",  # String, experiment name\n",
        "    use_control_codes = True,  # True/False, enable conditional text generation or do basic text generation\n",
        "    force_dataset_update = True, # True/False, enable database updates even if it is already present on the file system\n",
        "    control_codes_type = \"special_token\",  # \"special_token\"/\"separators\"\n",
        "    use_supercategories = True,  # True/False, add supercategories as control codes \n",
        "    use_categories = False, # True/False, add categories as control codes    \n",
        "    use_control_codes_powerset = False,  # True/False, use powerset of control codes for each caption to augment dataset\n",
        "    max_control_codes_per_caption = 3,  # positive integer, maximum number of control codes to use with one caption during training\n",
        "    limited_run = True, # if set to True, the datasets will be reduced in size\n",
        "    max_train_set_len = 1500,  # positive integer, maximum number of items for the training set used\n",
        "    max_val_set_len = 1000,  # positive integer, maximum number of items for the validation set used\n",
        "    model=\"gpt2\",  # we tested \"distilgpt2\" and \"gpt2\" for now\n",
        "    #save_model_path = \"OUTPUT\",\n",
        "    #random_seed = 42,  # integer, random seed used anywhere it could be useful to add some determinism\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=ctrl_dry_runs\n",
            "env: WANDB_ENTITY=polito_aiml2021_textgen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBhNFP3c9h-Y",
        "outputId": "90604e55-c3c2-4edd-8604-9c850724822d"
      },
      "source": [
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_WATCH=all\n",
        "%env WANDB_SILENT=true"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WANDB_LOG_MODEL=true\n",
            "env: WANDB_WATCH=all\n",
            "env: WANDB_SILENT=true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPSvgf5wfoWI"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./data/results\",  # output directory\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,  # total # of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./data/logs',  # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns=False\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdD-oyY-pZwO"
      },
      "source": [
        "#TODO integrations with drive for checkpoints? It would work only in colab... not on azure or locally... should be parametrized?\n",
        "\n",
        "#TODO integration with WandB"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUKg6vYrcBv8"
      },
      "source": [
        "# Dataset\n",
        "We download and load the COCO captions dataset.\n",
        "\n",
        "We join in a single item the caption for an image with the categories and/or supercategories associated to objects present in the image.\n",
        "Categories and/or supercategories are used as control codes depending on the experiment settings.\n",
        "\n",
        "The dataset is then post processed to train the model with different combinations of control codes for each caption, depending on the experiment parameters. The output of the postprocessing is saved on .txt files that are then loaded and further handled by the Dataset class provided by HuggingFace datasets (used for its performance and caching abilities).\n",
        "\n",
        "Here we start with the functions needed to download the COCO captions dataset and preprocess it for our use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcVIj08WcHMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc17ef1a-8553-45d3-95fb-3efb4514f415"
      },
      "source": [
        "#TODO: Should we move this to an external file? Probably not since it is interesting to show?\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess  # to run sh commands\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from itertools import chain, combinations, groupby\n",
        "\n",
        "!mkdir data\n",
        "DATA_PATH=\"./data\"\n",
        "\n",
        "def download_annotations_dataset(data_path=DATA_PATH):\n",
        "    # download only if don't have it already\n",
        "    if not os.path.isdir(os.path.join(data_path,\"annotations\")):\n",
        "        if not os.path.exists(data_path):\n",
        "            os.makedirs(data_path)\n",
        "        subprocess.run([\"wget\",\"-P\", data_path, \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"])\n",
        "        subprocess.run([\"unzip\", \"-d\", data_path, os.path.join(data_path,\"annotations_trainval2017.zip\")])\n",
        "\n",
        "def map_and_join_dataset(data_instances, data_captions):\n",
        "    if not experiment_parameters[\"use_categories\"] and not experiment_parameters[\"use_supercategories\"]:\n",
        "        print(\"One of categories and supercategories has to be used!\")\n",
        "        sys.exit()\n",
        "    categories_data_dict = dict(map(lambda c: (c[\"id\"], c), data_instances[\"categories\"])) # <category_id, category>\n",
        "    annotations_data_mapped = map(lambda c: (c[\"image_id\"], c), data_instances[\"annotations\"]) # <image_id, annotation>\n",
        "    annotations_data_dict = {}\n",
        "    for a in annotations_data_mapped:\n",
        "        if a[0] in annotations_data_dict:\n",
        "            annotations_data_dict[a[0]] += [a[1]]\n",
        "        else:\n",
        "            annotations_data_dict[a[0]] = [a[1]]\n",
        "    captions_data_list = list(map(lambda c: (c[\"image_id\"], c), data_captions[\"annotations\"]))\n",
        "    captions_data_dict = dict()\n",
        "    for image_id, image_captions in groupby(sorted(captions_data_list, key=lambda x: x[0]), lambda x: x[0]):\n",
        "      image_captions_dict = dict()\n",
        "      for caption in image_captions:\n",
        "        image_captions_dict[caption[1][\"id\"]]=caption[1]\n",
        "      captions_data_dict[image_id]=image_captions_dict\n",
        "\n",
        "    dataset = []\n",
        "    control_codes_dict = {}\n",
        "    no_category_counter = 0\n",
        "    references_dict = {}\n",
        "\n",
        "    for image_id, captions in captions_data_dict.items():\n",
        "        references_dict[image_id] = list(map(lambda x: x[1][\"caption\"], captions.items()))\n",
        "        for _, caption in captions.items():\n",
        "          item = {\"caption\": caption, \"categories\": [], \"image_id\": image_id}\n",
        "          if image_id in annotations_data_dict:\n",
        "              tmp_categories_dict = {}\n",
        "              for a in annotations_data_dict[image_id]:\n",
        "                  category_name = categories_data_dict[a[\"category_id\"]][\"name\"]\n",
        "                  supercategory_name = categories_data_dict[a[\"category_id\"]][\"supercategory\"]\n",
        "                  if experiment_parameters[\"use_categories\"]:\n",
        "                      tmp_categories_dict[category_name] = 1\n",
        "                      control_codes_dict[category_name] = 1\n",
        "                  if experiment_parameters[\"use_supercategories\"]:\n",
        "                    tmp_categories_dict[supercategory_name] = 1\n",
        "                    control_codes_dict[supercategory_name] = 1\n",
        "              item[\"categories\"]=list(tmp_categories_dict.keys())\n",
        "          if len(item[\"categories\"])==0:\n",
        "              no_category_counter += 1\n",
        "          else: dataset += [item]\n",
        "\n",
        "    #TODO compute total of captions?\n",
        "\n",
        "    print(\"There are \"+str(no_category_counter)+\" captions without a category\")\n",
        "    return dataset, references_dict, list(control_codes_dict.keys())\n",
        "\n",
        "def load_or_setup_dataset(data_path=DATA_PATH, split='train'):\n",
        "    if not split in ['train', 'val']:\n",
        "        print(\"Unknown split: \"+split)\n",
        "        sys.exit()\n",
        "    if not experiment_parameters[\"force_dataset_update\"] and os.path.isfile(os.path.join(data_path, \"dataset_with_ctrl_\"+split+\".json\")):\n",
        "        print (\"Dataset json file, loading dataset...\")\n",
        "        with open(os.path.join(data_path, \"dataset_with_ctrl_\"+split+\".json\"), \"r\") as read_file:\n",
        "            dataset = json.load(read_file)\n",
        "        with open(os.path.join(data_path, \"control_codes_\"+split+\".json\"), \"r\") as read_file:\n",
        "            control_codes = json.load(read_file)\n",
        "        with open(os.path.join(data_path, \"references_\"+split+\".json\"), \"r\") as read_file:\n",
        "            references_dict = json.load(read_file)\n",
        "    else:\n",
        "        print (\"Dataset json file does not exist, creating dataset from scratch...\")\n",
        "        download_annotations_dataset(data_path=data_path)\n",
        "        with open(os.path.join(data_path,\"annotations/instances_\"+split+\"2017.json\"), \"r\") as read_file:\n",
        "            data_instances = json.load(read_file)\n",
        "\n",
        "        with open(os.path.join(data_path,\"annotations/captions_\"+split+\"2017.json\"), \"r\") as read_file:\n",
        "            data_captions = json.load(read_file)\n",
        "\n",
        "        dataset, references_dict, control_codes = map_and_join_dataset(data_instances, data_captions)\n",
        "\n",
        "        with open(os.path.join(data_path,\"control_codes_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(control_codes, outfile)\n",
        "\n",
        "        with open(os.path.join(data_path,\"references_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(references_dict, outfile)\n",
        "        \n",
        "        with open(os.path.join(data_path,\"dataset_with_ctrl_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(dataset, outfile)\n",
        "    return dataset, references_dict, control_codes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTXHhsBfcf5"
      },
      "source": [
        "Actually call the functions previously defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn1Y5hvlcc5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa564ae3-b251-4a55-eb28-f56875b9c270"
      },
      "source": [
        "data_path=DATA_PATH\n",
        "\n",
        "dataset_train, _, categories = load_or_setup_dataset(data_path=data_path, split=\"train\")\n",
        "dataset_val, references, _ = load_or_setup_dataset(data_path=data_path, split=\"val\")\n",
        "\n",
        "print(\"There are \"+str(len(dataset_train))+\" captions considered in total (train)\")\n",
        "print(\"There are \"+str(len(dataset_val))+\" captions considered in total (val)\")\n",
        "\n",
        "print(\"The following \"+str(len(categories))+\" categories are present in the dataset:\")\n",
        "print(categories)\n",
        "\n",
        "if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "    control_codes = []\n",
        "    for category in categories:\n",
        "        control_codes += [\"<CTRL:\"+category.replace(\" \",\"_\")+\">\"]\n",
        "\n",
        "    print(\"Processed control codes:\")\n",
        "    print(control_codes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset json file does not exist, creating dataset from scratch...\n",
            "There are 5107 captions without a category\n",
            "Dataset json file does not exist, creating dataset from scratch...\n",
            "There are 240 captions without a category\n",
            "There are 586646 captions considered in total (train)\n",
            "There are 24774 captions considered in total (val)\n",
            "The following 12 categories are present in the dataset:\n",
            "['kitchen', 'food', 'animal', 'furniture', 'indoor', 'accessory', 'person', 'vehicle', 'outdoor', 'sports', 'appliance', 'electronic']\n",
            "Processed control codes:\n",
            "['<CTRL:kitchen>', '<CTRL:food>', '<CTRL:animal>', '<CTRL:furniture>', '<CTRL:indoor>', '<CTRL:accessory>', '<CTRL:person>', '<CTRL:vehicle>', '<CTRL:outdoor>', '<CTRL:sports>', '<CTRL:appliance>', '<CTRL:electronic>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMaQOJO5cfql"
      },
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "chunk_size = 500\n",
        "\n",
        "def powerset(iterable, max_size=None):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "    if max_size is None:\n",
        "        return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "    else:\n",
        "        return chain.from_iterable(combinations(s, r) for r in range(min(max_size, len(s)+1)))\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    chunk_number = chunk[0]\n",
        "    chunk_items = chunk[1]\n",
        "    data_path = chunk[2]\n",
        "    split = chunk[3]\n",
        "    json_file = os.path.join(data_path, \"captions_\"+split+\"_\"+str(chunk_number)+\".json\")\n",
        "    captions_array_for_json = []\n",
        "    for item in chunk_items:\n",
        "        if experiment_parameters[\"use_control_codes\"]:\n",
        "            if experiment_parameters[\"use_control_codes_powerset\"]:\n",
        "                control_codes_combinations = powerset(item['categories'], experiment_parameters[\"max_control_codes_per_caption\"])\n",
        "            else:\n",
        "                control_codes_combinations = [item['categories']]\n",
        "        else:\n",
        "            control_codes_combinations = [[]]\n",
        "        for control_codes_combination in control_codes_combinations:\n",
        "            pre_control_codes_string=\"\"\n",
        "            for category in sorted(control_codes_combination):\n",
        "                if experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "                    pre_control_codes_string+=\"<CTRL:\"+category.replace(\" \",\"_\")+\">\"\n",
        "                elif experiment_parameters[\"control_codes_type\"] == \"separators\":\n",
        "                    pre_control_codes_string+=category+\", \"\n",
        "                else:\n",
        "                    print(\"ERROR: wrong control code type\")\n",
        "                    return -1  # TODO here we could fail better\n",
        "            captions_array_for_json += [{\"caption\": pre_control_codes_string+'<|endoftext|>'+item[\"caption\"][\"caption\"]+'<|endoftext|>',\"image_id\": item[\"caption\"][\"image_id\"]}]\n",
        "    with open(json_file, 'w') as captions_json:\n",
        "        json.dump({\"data\": captions_array_for_json}, captions_json)\n",
        "\n",
        "\n",
        "def write_json_chunks(dataset, split, data_path, chunk_size):\n",
        "    chunks = [dataset[start:min(start+chunk_size,len(dataset))] for start in range(0, len(dataset), chunk_size)]\n",
        "    pool = mp.Pool(processes=8)\n",
        "    pool.map(process_chunk, [(chunk_n, chunk_items, data_path, split) for chunk_n, chunk_items in enumerate(chunks)])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBoWOFcGd1VS"
      },
      "source": [
        "write_json_chunks(dataset_train, \"train\", data_path, chunk_size)\n",
        "write_json_chunks(dataset_val, \"val\", data_path, chunk_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSeE_0ZOcif-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128,
          "referenced_widgets": [
            "6ac82f268e904a999854178e4c59fb2d",
            "5156f4c310e74a3798e2ec9c14f1f3f4",
            "5ba84f66dd8841269707b1a01af199c2",
            "dc86b11a50684a8aa7ee651cf9705731",
            "f05aea6e898e4ab9991f1fef205ccc38",
            "6db73283afa9422ab02de5995547c1ce",
            "a4124e284cde4a18a8ca8af0593e9d94",
            "e2507ec0b10e42f5acb297844b661330",
            "3483300124ca4ca6b9bcedea43fdad04",
            "cd65a98b298f4f1f95df35c993a57b5e",
            "a1db641052984f4681b2c805d38ff027",
            "f8109a8db5a94027b1ba4129148106f9",
            "e41b4f3561c64136a0a014ebfe042445",
            "a43412adf447443f808c9fb7645a0bce",
            "3ca9381c022a40dd904ba9d673cd5fa4",
            "feb3df4c3fba4c16bdfc92b14616fbf6"
          ]
        },
        "outputId": "b401c06b-a8a2-4b18-846d-84efac5e45d4"
      },
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import glob\n",
        "\n",
        "dataset_train, dataset_val = load_dataset('json', data_files={'train': glob.glob('./data/captions_train_*.json'), 'val': glob.glob('./data/captions_val_*.json')}, split=['train', 'val'], field=\"data\")\n",
        "print(\"Augmented dataset has: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "\n",
        "if experiment_parameters[\"limited_run\"]: # shuffle and cut the datasets\n",
        "  dataset_train = dataset_train.shuffle(42).select(range(experiment_parameters[\"max_train_set_len\"]))\n",
        "  dataset_val = dataset_val.shuffle(42).select(range(experiment_parameters[\"max_val_set_len\"]))\n",
        "  print(\"We take only a small part of that: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "else: # just shuffle them\n",
        "  dataset_train = dataset_train.shuffle(42)\n",
        "  dataset_val = dataset_val.shuffle(42)\n",
        "  print(\"Train elements: \"+str(len(dataset_train))+\"\\nValidation elements: \"+str(len(dataset_val)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-5aec630f072696ca\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-5aec630f072696ca/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ac82f268e904a999854178e4c59fb2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3483300124ca4ca6b9bcedea43fdad04",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5aec630f072696ca/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
            "Augmented dataset has: 586646 train elements and 24774 validation elements\n",
            "We take only a small part of that: 1500 train elements and 1000 validation elements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqZ10H3dTIw"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCANYNf9XWet",
        "outputId": "256b2069-06ea-448b-d1fe-ade43816cf83"
      },
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(experiment_parameters['model'])\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer before added special tokens \"+str(len(tokenizer)))\n",
        "\n",
        "if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "    special_tokens_dict = {'additional_special_tokens': control_codes}\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print(\"added \"+str(num_added_toks)+\" tokens to the pretrained tokenizer\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer before added special tokens 50257\n",
            "added 12 tokens to the pretrained tokenizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "c591a13ea904480e93e1a3ed89962653",
            "d10fc94ed2234b9a8fb4dec3f77d1bc1",
            "7648849dfcd041a39fc1fbdccaa912aa",
            "2de2324177f8494d85bcf2aed59223f2",
            "c2112e489f684fa9a1931531b4b06aea",
            "e00e4b93b75040f5b952ac8d4a819431",
            "65393a1757db4f2498513a38386a8e35",
            "95e2d7312d264e78a0b3b0011ba92e7c",
            "bbd0f88ccd79446fa06d04b2a6586eba",
            "41340038de34465f93b28d53b2e5a7ab",
            "80934554cef74c3cbab3b0393dc351cf",
            "71a81f726adc46328fb348ca3a86b555",
            "03a6c292555940839096fbb27d8dc2e3",
            "6f0ae8a8f77e4ae78cd9ee3b0b196ce4",
            "c74c13676d694bc18a4230079c0ae9ce",
            "202d415297c1414ab1ef65e44e550caf"
          ]
        },
        "id": "UpJ76wxIXXQq",
        "outputId": "5ef18cb9-e584-4961-d635-21e60a306a68"
      },
      "source": [
        "def encode(examples):\n",
        "    encoded = tokenizer(examples['caption'], truncation=True, max_length=64, padding=\"max_length\")\n",
        "    encoded['labels'] = encoded['input_ids']\n",
        "    encoded['image_id'] = examples['image_id']\n",
        "    return encoded\n",
        "\n",
        "dataset_train_encoded = dataset_train.map(encode, batched=True)\n",
        "dataset_val_encoded = dataset_val.map(encode, batched=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c591a13ea904480e93e1a3ed89962653",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbd0f88ccd79446fa06d04b2a6586eba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwzZXNKdZ0H"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFMrleG6XZ7l",
        "outputId": "85987fd4-0d65-4f83-9f5d-c5d17e1807f9"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(experiment_parameters['model'], pad_token_id=tokenizer.eos_token_id)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50269, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7QMoShFuc10"
      },
      "source": [
        "#TODO add the possibility to freeze some layers? Add an experiment parameter for this?\n",
        "#TODO print the model structure?"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ape3DYdcJ0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvJTYqvEXe0e"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePKXiMzX3hl"
      },
      "source": [
        "import datasets\n",
        "\n",
        "def compute_metrics(pred, image_ids):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions\n",
        "  metric = datasets.load_metric('sacrebleu')\n",
        "\n",
        "  preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  references_local_list = [references[image_id.item()] for image_id in image_ids]\n",
        "\n",
        "  final_score = metric.compute(predictions=preds, references=references_local_list)\n",
        "  return {\n",
        "      'bleu': final_score\n",
        "  }"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH1NnAXsX-i1"
      },
      "source": [
        "#TODO: Should we move this to an external file?\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "import collections\n",
        "import inspect\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "import warnings\n",
        "from logging import StreamHandler\n",
        "from pathlib import Path\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Integrations must be imported before ML frameworks:\n",
        "from transformers.integrations import (  # isort: split\n",
        "    default_hp_search_backend,\n",
        "    get_reporting_integration_callbacks,\n",
        "    hp_params,\n",
        "    is_fairscale_available,\n",
        "    is_optuna_available,\n",
        "    is_ray_tune_available,\n",
        "    run_hp_search_optuna,\n",
        "    run_hp_search_ray,\n",
        "    deepspeed_init,\n",
        "    is_deepspeed_zero3_enabled,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import __version__\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
        "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
        "from transformers.dependency_versions_check import dep_version_check\n",
        "from transformers.file_utils import (\n",
        "    CONFIG_NAME,\n",
        "    WEIGHTS_NAME,\n",
        "    PushToHubMixin,\n",
        "    is_apex_available,\n",
        "    is_datasets_available,\n",
        "    is_in_notebook,\n",
        "    is_sagemaker_dp_enabled,\n",
        "    is_sagemaker_mp_enabled,\n",
        "    is_torch_tpu_available,\n",
        "    is_training_run_on_sagemaker,\n",
        ")\n",
        "from transformers.modelcard import TrainingSummary\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "from transformers.optimization import Adafactor, AdamW, get_scheduler\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.trainer_callback import (\n",
        "    CallbackHandler,\n",
        "    DefaultFlowCallback,\n",
        "    PrinterCallback,\n",
        "    ProgressCallback,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState,\n",
        ")\n",
        "from transformers.trainer_pt_utils import (\n",
        "    DistributedLengthGroupedSampler,\n",
        "    DistributedSamplerWithLoop,\n",
        "    DistributedTensorGatherer,\n",
        "    IterableDatasetShard,\n",
        "    LabelSmoother,\n",
        "    LengthGroupedSampler,\n",
        "    SequentialDistributedSampler,\n",
        "    ShardSampler,\n",
        "    distributed_broadcast_scalars,\n",
        "    distributed_concat,\n",
        "    find_batch_size,\n",
        "    get_parameter_names,\n",
        "    nested_concat,\n",
        "    nested_detach,\n",
        "    nested_numpify,\n",
        "    nested_truncate,\n",
        "    nested_xla_mesh_reduce,\n",
        "    reissue_pt_warnings,\n",
        ")\n",
        "from transformers.trainer_utils import (\n",
        "    PREFIX_CHECKPOINT_DIR,\n",
        "    BestRun,\n",
        "    EvalLoopOutput,\n",
        "    EvalPrediction,\n",
        "    HPSearchBackend,\n",
        "    PredictionOutput,\n",
        "    ShardedDDPOption,\n",
        "    TrainerMemoryTracker,\n",
        "    TrainOutput,\n",
        "    default_compute_objective,\n",
        "    default_hp_space,\n",
        "    denumpify_detensorize,\n",
        "    get_last_checkpoint,\n",
        "    set_seed,\n",
        "    speed_metrics,\n",
        ")\n",
        "from transformers.training_args import ParallelMode, TrainingArguments\n",
        "from transformers.utils import logging\n",
        "from transformers.utils.modeling_auto_mapping import MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES\n",
        "\n",
        "\n",
        "_is_torch_generator_available = False\n",
        "_is_native_amp_available = False\n",
        "\n",
        "DEFAULT_CALLBACKS = [DefaultFlowCallback]\n",
        "DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n",
        "\n",
        "if is_in_notebook():\n",
        "    from transformers.utils.notebook import NotebookProgressCallback\n",
        "\n",
        "    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n",
        "\n",
        "if is_apex_available():\n",
        "    from apex import amp\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
        "    _is_torch_generator_available = True\n",
        "    _is_native_amp_available = True\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "if is_datasets_available():\n",
        "    import datasets\n",
        "\n",
        "if is_torch_tpu_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.debug.metrics as met\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "if is_fairscale_available():\n",
        "    dep_version_check(\"fairscale\")\n",
        "    import fairscale\n",
        "    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n",
        "    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n",
        "    from fairscale.nn.wrap import auto_wrap\n",
        "    from fairscale.optim import OSS\n",
        "    from fairscale.optim.grad_scaler import ShardedGradScaler\n",
        "\n",
        "if is_sagemaker_dp_enabled():\n",
        "    import smdistributed.dataparallel.torch.distributed as dist\n",
        "    from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
        "else:\n",
        "    import torch.distributed as dist\n",
        "\n",
        "if is_sagemaker_mp_enabled():\n",
        "    import smdistributed.modelparallel.torch as smp\n",
        "\n",
        "    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n",
        "\n",
        "if is_training_run_on_sagemaker():\n",
        "    logging.add_handler(StreamHandler(sys.stdout))\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    import optuna\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def evaluation_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str,\n",
        "        prediction_loss_only: Optional[bool] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> EvalLoopOutput:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
        "\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        prediction_loss_only = (\n",
        "            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only\n",
        "        )\n",
        "\n",
        "        # if eval is called w/o train init deepspeed here\n",
        "        if self.args.deepspeed and not self.deepspeed:\n",
        "\n",
        "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
        "            # from the checkpoint eventually\n",
        "            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n",
        "            self.model = deepspeed_engine.module\n",
        "            self.model_wrapped = deepspeed_engine\n",
        "            self.deepspeed = deepspeed_engine\n",
        "            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n",
        "            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n",
        "            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n",
        "            deepspeed_engine.optimizer.optimizer = None\n",
        "            deepspeed_engine.lr_scheduler = None\n",
        "\n",
        "        model = self._wrap_model(self.model, training=False)\n",
        "\n",
        "        # if full fp16 is wanted on eval and this ``evaluation`` or ``predict`` isn't called while\n",
        "        # ``train`` is running, halve it first and then put on device\n",
        "        if not self.is_in_train and self.args.fp16_full_eval:\n",
        "            model = model.half().to(self.args.device)\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(f\"***** Running {description} *****\")\n",
        "        if isinstance(dataloader.dataset, collections.abc.Sized):\n",
        "            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n",
        "        else:\n",
        "            logger.info(\"  Num examples: Unknown\")\n",
        "        logger.info(f\"  Batch size = {batch_size}\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        self.callback_handler.eval_dataloader = dataloader\n",
        "        # Do this before wrapping.\n",
        "        eval_dataset = dataloader.dataset\n",
        "\n",
        "        if is_torch_tpu_available():\n",
        "            dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
        "\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = None\n",
        "\n",
        "        # Initialize containers\n",
        "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
        "        losses_host = None\n",
        "        preds_host = None\n",
        "        labels_host = None\n",
        "        # losses/preds/labels on CPU (final containers)\n",
        "        all_losses = None\n",
        "        all_preds = None\n",
        "        all_labels = None\n",
        "        # Will be useful when we have an iterable dataset so don't know its length.\n",
        "\n",
        "        observed_num_examples = 0\n",
        "        # Main evaluation loop\n",
        "        for step, inputs in enumerate(dataloader):\n",
        "            # Update the observed num examples\n",
        "            observed_batch_size = find_batch_size(inputs)\n",
        "            if observed_batch_size is not None:\n",
        "                observed_num_examples += observed_batch_size\n",
        "\n",
        "            # Prediction step\n",
        "            if isinstance(inputs, list):\n",
        "                inputs_for_prediction = [dict(filter(lambda i: i[0]!='image_id', input.items())) for input in inputs]\n",
        "            else:\n",
        "                inputs_for_prediction = dict(filter(lambda i: i[0]!='image_id', inputs.items()))\n",
        "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
        "\n",
        "            # Update containers on host\n",
        "            if loss is not None:\n",
        "                losses = self._nested_gather(loss.repeat(batch_size))\n",
        "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
        "            ############################\n",
        "            if logits is not None:\n",
        "                logits = self._pad_across_processes(logits)\n",
        "                logits = self._nested_gather(logits)\n",
        "                logits_reduced = np.argmax(logits.cpu(), axis=-1) # Obtain a single value instead of a vector, for memory efficiency\n",
        "                preds_host = logits_reduced if preds_host is None else nested_concat(preds_host, logits_reduced, padding_index=-100)\n",
        "                # preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
        "            ############################\n",
        "            if labels is not None:\n",
        "                labels = self._pad_across_processes(labels)\n",
        "                labels = self._nested_gather(labels)\n",
        "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
        "            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)\n",
        "\n",
        "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
        "            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:\n",
        "                if losses_host is not None:\n",
        "                    losses = nested_numpify(losses_host)\n",
        "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "                if preds_host is not None:\n",
        "                    logits = nested_numpify(preds_host)\n",
        "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "                if labels_host is not None:\n",
        "                    labels = nested_numpify(labels_host)\n",
        "                    all_labels = (\n",
        "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "                    )\n",
        "\n",
        "                # Set back to None to begin a new accumulation\n",
        "                losses_host, preds_host, labels_host = None, None, None\n",
        "\n",
        "        if self.args.past_index and hasattr(self, \"_past\"):\n",
        "            # Clean the state at the end of the evaluation loop\n",
        "            delattr(self, \"_past\")\n",
        "\n",
        "        # Gather all remaining tensors and put them back on the CPU\n",
        "        if losses_host is not None:\n",
        "            losses = nested_numpify(losses_host)\n",
        "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "        if preds_host is not None:\n",
        "            logits = nested_numpify(preds_host)\n",
        "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "        if labels_host is not None:\n",
        "            labels = nested_numpify(labels_host)\n",
        "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "\n",
        "        # Number of samples\n",
        "        if not isinstance(eval_dataset, IterableDataset):\n",
        "            num_samples = len(eval_dataset)\n",
        "        elif isinstance(eval_dataset, IterableDatasetShard):\n",
        "            num_samples = eval_dataset.num_examples\n",
        "        else:\n",
        "            num_samples = observed_num_examples\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses is not None:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds is not None:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels is not None:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "\n",
        "        if isinstance(inputs, list):\n",
        "            image_ids = [input[\"image_id\"] for input in inputs]\n",
        "        else:\n",
        "            image_ids = inputs[\"image_id\"]\n",
        "        \n",
        "        # Metrics!\n",
        "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
        "            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels), image_ids)\n",
        "        else:\n",
        "            metrics = {}\n",
        "\n",
        "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
        "        metrics = denumpify_detensorize(metrics)\n",
        "\n",
        "        if all_losses is not None:\n",
        "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJPAhYqXfhk"
      },
      "source": [
        "dataset_train_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dataset_val_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'image_id'])\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=dataset_train_encoded,         # training dataset\n",
        "    eval_dataset=dataset_val_encoded,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HodDpeuIXli2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8e2e5a18-f754-4772-8b0a-3ba0d56c4205"
      },
      "source": [
        "trainer.train()\n",
        "\n",
        "config = wandb.config\n",
        "config.update(experiment_parameters)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">./data/results</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/polito_aiml2021_textgen/ctrl_dry_runs\" target=\"_blank\">https://wandb.ai/polito_aiml2021_textgen/ctrl_dry_runs</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/polito_aiml2021_textgen/ctrl_dry_runs/runs/1jhzdus1\" target=\"_blank\">https://wandb.ai/polito_aiml2021_textgen/ctrl_dry_runs/runs/1jhzdus1</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210526_213442-1jhzdus1</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/72 00:29 < 01:00, 0.78 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8d2f7fec44ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2006\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         )\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d7463ab4bb57>\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0minputs_for_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# Update containers on host\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2354\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'image_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F_val6rXooF"
      },
      "source": [
        "trainer.save_model(\"./data/results\")\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}