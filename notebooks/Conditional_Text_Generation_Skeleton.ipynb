{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Text Generation Skeleton.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlqjjRTw2Pg5dNAcQ5jDcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarinelli/conditional_text_generation/blob/main/notebooks/Conditional_Text_Generation_Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-m_5f57otwG"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckZjerSAcpG"
      },
      "source": [
        "# Import utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzkWhU8JmWzD"
      },
      "source": [
        "!rm -r conditional_text_generation\n",
        "!git clone https://github.com/lucarinelli/conditional_text_generation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E2bk-hMmd2v"
      },
      "source": [
        "!pip install import-ipynb\n",
        "\n",
        "import import_ipynb\n",
        "\n",
        "print(\"Importing generic utilities\")\n",
        "from CtrlUtilities import *\n",
        "print(\"Generic utilities imported\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omx8BlQIqsE8"
      },
      "source": [
        "# Basic Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29pKQDSBqudu"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "experiment_parameters[\"run_name\"] = \"exp1\"  # String, experiment name\n",
        "experiment_parameters[\"use_control_codes\"] = True  # True/False, enable conditional text generation or do basic text generation\n",
        "experiment_parameters[\"force_dataset_update\"] = False # True/False, enable database updates even if it is already present on the file system\n",
        "experiment_parameters[\"control_codes_type\"] = \"special_token\" # \"special_token\"/\"separators\"\n",
        "experiment_parameters[\"use_supercategories\"] = True  # True/False, add supercategories as control codes \n",
        "experiment_parameters[\"use_categories\"] = False # True/False, add categories as control codes    \n",
        "experiment_parameters[\"use_control_codes_powerset\"] = False  # True/False, use powerset of control codes for each caption to augment dataset\n",
        "experiment_parameters[\"max_control_codes_per_caption\"] = 3  # positive integer, maximum number of control codes to use with one caption during training\n",
        "experiment_parameters[\"limited_run\"] = True # if set to True, the datasets will be reduced in size\n",
        "experiment_parameters[\"max_train_set_len\"] = 12  # positive integer, maximum number of items for the training set used\n",
        "experiment_parameters[\"max_val_set_len\"] = 12  # positive integer, maximum number of items for the validation set used\n",
        "experiment_parameters[\"model\"]= \"gpt2\"  # we tested \"distilgpt2\" and \"gpt2\" for now\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./data/results\",  # output directory\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,  # total # of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./data/logs',  # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "experiment_parameters[\"training_args\"] = training_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNbqhC2o3NC"
      },
      "source": [
        "# WanDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhXZM9Puoy42"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "%env WANDB_PROJECT=ctrl_dry_runs\n",
        "%env WANDB_ENTITY=polito_aiml2021_textgen\n",
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_WATCH=all\n",
        "%env WANDB_SILENT=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3K_XE_xsux8"
      },
      "source": [
        "# Basic Skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFpe02znrIk1"
      },
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import glob\n",
        "from transformers import GPT2TokenizerFast\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def initialize_generation():\n",
        "#######################################\n",
        "  #        Dataset\n",
        "#######################################\n",
        "  !mkdir data\n",
        "  DATA_PATH=\"./data\"\n",
        "\n",
        "  data_path=DATA_PATH\n",
        "\n",
        "  dataset_train, _, categories = load_or_setup_dataset(data_path=data_path, split=\"train\")\n",
        "  dataset_val, references, _ = load_or_setup_dataset(data_path=data_path, split=\"val\")\n",
        "\n",
        "  print(\"There are \"+str(len(dataset_train))+\" captions considered in total (train)\")\n",
        "  print(\"There are \"+str(len(dataset_val))+\" captions considered in total (val)\")\n",
        "\n",
        "  print(\"The following \"+str(len(categories))+\" categories are present in the dataset:\")\n",
        "  print(categories)\n",
        "\n",
        "  if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "      control_codes = []\n",
        "      for category in categories:\n",
        "          control_codes += [\"<CTRL:\"+category.replace(\" \",\"_\")+\">\"]\n",
        "\n",
        "      print(\"Processed control codes:\")\n",
        "      print(control_codes)\n",
        "\n",
        "#######################################\n",
        "  write_json_chunks(dataset_train, \"train\", data_path, chunk_size)\n",
        "  write_json_chunks(dataset_val, \"val\", data_path, chunk_size)\n",
        "#######################################\n",
        "  \n",
        "  dataset_train, dataset_val = load_dataset('json', data_files={'train': glob.glob('./data/captions_train_*.json'), 'val': glob.glob('./data/captions_val_*.json')}, split=['train', 'val'], field=\"data\")\n",
        "  print(\"Augmented dataset has: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "\n",
        "  if experiment_parameters[\"limited_run\"]: # shuffle and cut the datasets\n",
        "    dataset_train = dataset_train.shuffle(42).select(range(experiment_parameters[\"max_train_set_len\"]))\n",
        "    dataset_val = dataset_val.shuffle(42).select(range(experiment_parameters[\"max_val_set_len\"]))\n",
        "    print(\"We take only a small part of that: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "  else: # just shuffle them\n",
        "    dataset_train = dataset_train.shuffle(42)\n",
        "    dataset_val = dataset_val.shuffle(42)\n",
        "    print(\"Train elements: \"+str(len(dataset_train))+\"\\nValidation elements: \"+str(len(dataset_val)))\n",
        "#######################################\n",
        " ##         Tokenization\n",
        " #######################################\n",
        "\n",
        "  tokenizer = GPT2TokenizerFast.from_pretrained(experiment_parameters['model'])\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  print(\"Tokenizer before added special tokens \"+str(len(tokenizer)))\n",
        "\n",
        "  if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "      special_tokens_dict = {'additional_special_tokens': control_codes}\n",
        "      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "      print(\"added \"+str(num_added_toks)+\" tokens to the pretrained tokenizer\")\n",
        "#######################################\n",
        "\n",
        "  dataset_train_encoded = dataset_train.map(lambda x : encode(tokenizer, x), batched=True)\n",
        "  dataset_val_encoded = dataset_val.map(lambda x : encode(tokenizer, x), batched=True)\n",
        "\n",
        "#######################################\n",
        "#         Training\n",
        "#######################################\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "  dataset_train_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "  dataset_val_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'image_id'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV3VetkusQOL"
      },
      "source": [
        "def after_training(trainer):\n",
        "  config = wandb.config\n",
        "  config.update(experiment_parameters)\n",
        "  trainer.save_model(experiment_parameters[\"training_args\"].output_dir)\n",
        "  wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}