{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Text Generation Skeleton.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNQMMPsxVXUcHfCkv3cGT9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarinelli/conditional_text_generation/blob/main/notebooks/Conditional_Text_Generation_Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-m_5f57otwG"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrhHqQapB5f"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GdjG_Gco7AD"
      },
      "source": [
        "experiment_parameters = dict(\n",
        "    run_name = \"exp1\",  # String, experiment name\n",
        "    use_control_codes = True,  # True/False, enable conditional text generation or do basic text generation\n",
        "    force_dataset_update = False, # True/False, enable database updates even if it is already present on the file system\n",
        "    control_codes_type = \"special_token\",  # \"special_token\"/\"separators\"\n",
        "    use_supercategories = True,  # True/False, add supercategories as control codes \n",
        "    use_categories = False, # True/False, add categories as control codes    \n",
        "    use_control_codes_powerset = False,  # True/False, use powerset of control codes for each caption to augment dataset\n",
        "    max_control_codes_per_caption = 3,  # positive integer, maximum number of control codes to use with one caption during training\n",
        "    limited_run = True, # if set to True, the datasets will be reduced in size\n",
        "    max_train_set_len = 1500,  # positive integer, maximum number of items for the training set used\n",
        "    max_val_set_len = 1000,  # positive integer, maximum number of items for the validation set used\n",
        "    model=\"gpt2\",  # we tested \"distilgpt2\" and \"gpt2\" for now\n",
        "    #save_model_path = \"OUTPUT\",\n",
        "    #random_seed = 42,  # integer, random seed used anywhere it could be useful to add some determinism\n",
        ")\n",
        "\n",
        "%env experiment_parameters = experiment_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckZjerSAcpG"
      },
      "source": [
        "# Import utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzkWhU8JmWzD"
      },
      "source": [
        "!rm -r conditional_text_generation\n",
        "!git clone https://github.com/lucarinelli/conditional_text_generation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E2bk-hMmd2v"
      },
      "source": [
        "!pip install import-ipynb\n",
        "\n",
        "%cd conditional_text_generation/notebooks\n",
        "\n",
        "import import_ipynb\n",
        "from CtrlUtilities import *\n",
        "\n",
        "%cd ../.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNbqhC2o3NC"
      },
      "source": [
        "# WanDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhXZM9Puoy42"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "%env WANDB_PROJECT=ctrl_dry_runs\n",
        "%env WANDB_ENTITY=polito_aiml2021_textgen\n",
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_WATCH=all\n",
        "%env WANDB_SILENT=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CpPAzQhpNjR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-1C2JjtpO73"
      },
      "source": [
        "!mkdir data\n",
        "DATA_PATH=\"./data\"\n",
        "\n",
        "data_path=DATA_PATH\n",
        "\n",
        "dataset_train, _, categories = load_or_setup_dataset(data_path=data_path, split=\"train\")\n",
        "dataset_val, references, _ = load_or_setup_dataset(data_path=data_path, split=\"val\")\n",
        "\n",
        "print(\"There are \"+str(len(dataset_train))+\" captions considered in total (train)\")\n",
        "print(\"There are \"+str(len(dataset_val))+\" captions considered in total (val)\")\n",
        " \n",
        "print(\"The following \"+str(len(categories))+\" categories are present in the dataset:\")\n",
        "print(categories)\n",
        "\n",
        "if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "    control_codes = []\n",
        "    for category in categories:\n",
        "        control_codes += [\"<CTRL:\"+category.replace(\" \",\"_\")+\">\"]\n",
        "\n",
        "    print(\"Processed control codes:\")\n",
        "    print(control_codes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae3Mnr2npPrK"
      },
      "source": [
        "write_json_chunks(dataset_train, \"train\", data_path, chunk_size)\n",
        "write_json_chunks(dataset_val, \"val\", data_path, chunk_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysJNbI1DpTjR"
      },
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import glob\n",
        "\n",
        "dataset_train, dataset_val = load_dataset('json', data_files={'train': glob.glob('./data/captions_train_*.json'), 'val': glob.glob('./data/captions_val_*.json')}, split=['train', 'val'], field=\"data\")\n",
        "print(\"Augmented dataset has: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "\n",
        "if experiment_parameters[\"limited_run\"]: # shuffle and cut the datasets\n",
        "  dataset_train = dataset_train.shuffle(42).select(range(experiment_parameters[\"max_train_set_len\"]))\n",
        "  dataset_val = dataset_val.shuffle(42).select(range(experiment_parameters[\"max_val_set_len\"]))\n",
        "  print(\"We take only a small part of that: \"+str(len(dataset_train))+\" train elements and \"+str(len(dataset_val))+\" validation elements\")\n",
        "else: # just shuffle them\n",
        "  dataset_train = dataset_train.shuffle(42)\n",
        "  dataset_val = dataset_val.shuffle(42)\n",
        "  print(\"Train elements: \"+str(len(dataset_train))+\"\\nValidation elements: \"+str(len(dataset_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cdIJ7p4pV9F"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpW0S3UhpXPh"
      },
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(experiment_parameters['model'])\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer before added special tokens \"+str(len(tokenizer)))\n",
        "\n",
        "if experiment_parameters[\"use_control_codes\"] and experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "    special_tokens_dict = {'additional_special_tokens': control_codes}\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print(\"added \"+str(num_added_toks)+\" tokens to the pretrained tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_jrLmQpZnk"
      },
      "source": [
        "dataset_train_encoded = dataset_train.map(encode, batched=True)\n",
        "dataset_val_encoded = dataset_val.map(encode, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP0-9BW0pbZI"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6koe9WZpcKe"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(experiment_parameters['model'], pad_token_id=tokenizer.eos_token_id)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wygHEeQ-pcgb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZZzWFYgpK5L"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./data/results\",  # output directory\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,  # total # of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./data/logs',  # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzOo5iNjpgZx"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd3a3yd0pl4L"
      },
      "source": [
        "dataset_train_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dataset_val_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'image_id'])\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=dataset_train_encoded,         # training dataset\n",
        "    eval_dataset=dataset_val_encoded,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDSXK8-epna5"
      },
      "source": [
        "trainer.train()\n",
        "\n",
        "config = wandb.config\n",
        "config.update(experiment_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99rrbSCSpo1d"
      },
      "source": [
        "trainer.save_model(\"./data/results\")\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}