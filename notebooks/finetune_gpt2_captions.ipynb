{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset json file, loading dataset...\n",
      "There are 118287 captions in total (train)\n",
      "The following 91 categories are present in the dataset:\n",
      "['bicycle', 'vehicle', 'clock', 'indoor', 'cup', 'kitchen', 'sink', 'appliance', 'vase', 'car', 'motorcycle', 'person', 'bench', 'outdoor', 'airplane', 'bottle', 'toilet', 'furniture', 'potted plant', 'toothbrush', 'refrigerator', 'oven', 'apple', 'food', 'banana', 'surfboard', 'sports', 'bowl', 'spoon', 'traffic light', 'cat', 'animal', 'cow', 'handbag', 'accessory', 'umbrella', 'book', 'skateboard', 'horse', 'cake', 'donut', 'chair', 'cell phone', 'electronic', 'laptop', 'mouse', 'tv', 'dog', 'knife', 'orange', 'microwave', 'backpack', 'bus', 'dining table', 'truck', 'bird', 'giraffe', 'suitcase', 'boat', 'skis', 'fire hydrant', 'toaster', 'wine glass', 'sheep', 'remote', 'scissors', 'tie', 'kite', 'broccoli', 'stop sign', 'teddy bear', 'fork', 'keyboard', 'elephant', 'carrot', 'parking meter', 'snowboard', 'pizza', 'baseball glove', 'frisbee', 'train', 'couch', 'zebra', 'hair drier', 'bed', 'sandwich', 'baseball bat', 'sports ball', 'hot dog', 'tennis racket', 'bear']\n",
      "Processed control codes:\n",
      "['<CTRL:bicycle>', '<CTRL:vehicle>', '<CTRL:clock>', '<CTRL:indoor>', '<CTRL:cup>', '<CTRL:kitchen>', '<CTRL:sink>', '<CTRL:appliance>', '<CTRL:vase>', '<CTRL:car>', '<CTRL:motorcycle>', '<CTRL:person>', '<CTRL:bench>', '<CTRL:outdoor>', '<CTRL:airplane>', '<CTRL:bottle>', '<CTRL:toilet>', '<CTRL:furniture>', '<CTRL:potted_plant>', '<CTRL:toothbrush>', '<CTRL:refrigerator>', '<CTRL:oven>', '<CTRL:apple>', '<CTRL:food>', '<CTRL:banana>', '<CTRL:surfboard>', '<CTRL:sports>', '<CTRL:bowl>', '<CTRL:spoon>', '<CTRL:traffic_light>', '<CTRL:cat>', '<CTRL:animal>', '<CTRL:cow>', '<CTRL:handbag>', '<CTRL:accessory>', '<CTRL:umbrella>', '<CTRL:book>', '<CTRL:skateboard>', '<CTRL:horse>', '<CTRL:cake>', '<CTRL:donut>', '<CTRL:chair>', '<CTRL:cell_phone>', '<CTRL:electronic>', '<CTRL:laptop>', '<CTRL:mouse>', '<CTRL:tv>', '<CTRL:dog>', '<CTRL:knife>', '<CTRL:orange>', '<CTRL:microwave>', '<CTRL:backpack>', '<CTRL:bus>', '<CTRL:dining_table>', '<CTRL:truck>', '<CTRL:bird>', '<CTRL:giraffe>', '<CTRL:suitcase>', '<CTRL:boat>', '<CTRL:skis>', '<CTRL:fire_hydrant>', '<CTRL:toaster>', '<CTRL:wine_glass>', '<CTRL:sheep>', '<CTRL:remote>', '<CTRL:scissors>', '<CTRL:tie>', '<CTRL:kite>', '<CTRL:broccoli>', '<CTRL:stop_sign>', '<CTRL:teddy_bear>', '<CTRL:fork>', '<CTRL:keyboard>', '<CTRL:elephant>', '<CTRL:carrot>', '<CTRL:parking_meter>', '<CTRL:snowboard>', '<CTRL:pizza>', '<CTRL:baseball_glove>', '<CTRL:frisbee>', '<CTRL:train>', '<CTRL:couch>', '<CTRL:zebra>', '<CTRL:hair_drier>', '<CTRL:bed>', '<CTRL:sandwich>', '<CTRL:baseball_bat>', '<CTRL:sports_ball>', '<CTRL:hot_dog>', '<CTRL:tennis_racket>', '<CTRL:bear>']\n",
      "Writing txt\n",
      "txt file already exists, nothing to do here...\n",
      "No tokenizer provided, you will need to provide one later through the tokenize method!\n",
      "Dataset json file, loading dataset...\n",
      "There are 5000 captions in total (val)\n",
      "The following 91 categories are present in the dataset:\n",
      "['motorcycle', 'vehicle', 'tv', 'electronic', 'chair', 'furniture', 'laptop', 'mouse', 'keyboard', 'backpack', 'accessory', 'sports ball', 'sports', 'bottle', 'kitchen', 'potted plant', 'book', 'indoor', 'vase', 'cup', 'toilet', 'sink', 'appliance', 'car', 'person', 'bench', 'outdoor', 'handbag', 'truck', 'spoon', 'cake', 'food', 'bowl', 'dining table', 'bird', 'animal', 'cat', 'clock', 'cell phone', 'remote', 'bed', 'microwave', 'oven', 'banana', 'dog', 'couch', 'giraffe', 'cow', 'airplane', 'traffic light', 'refrigerator', 'orange', 'apple', 'train', 'bicycle', 'zebra', 'kite', 'scissors', 'knife', 'umbrella', 'horse', 'toaster', 'fork', 'sheep', 'teddy bear', 'bus', 'fire hydrant', 'sandwich', 'stop sign', 'suitcase', 'tie', 'toothbrush', 'skateboard', 'parking meter', 'frisbee', 'skis', 'carrot', 'donut', 'wine glass', 'surfboard', 'boat', 'hair drier', 'pizza', 'snowboard', 'broccoli', 'hot dog', 'elephant', 'baseball bat', 'baseball glove', 'bear', 'tennis racket']\n",
      "Processed control codes:\n",
      "['<CTRL:motorcycle>', '<CTRL:vehicle>', '<CTRL:tv>', '<CTRL:electronic>', '<CTRL:chair>', '<CTRL:furniture>', '<CTRL:laptop>', '<CTRL:mouse>', '<CTRL:keyboard>', '<CTRL:backpack>', '<CTRL:accessory>', '<CTRL:sports_ball>', '<CTRL:sports>', '<CTRL:bottle>', '<CTRL:kitchen>', '<CTRL:potted_plant>', '<CTRL:book>', '<CTRL:indoor>', '<CTRL:vase>', '<CTRL:cup>', '<CTRL:toilet>', '<CTRL:sink>', '<CTRL:appliance>', '<CTRL:car>', '<CTRL:person>', '<CTRL:bench>', '<CTRL:outdoor>', '<CTRL:handbag>', '<CTRL:truck>', '<CTRL:spoon>', '<CTRL:cake>', '<CTRL:food>', '<CTRL:bowl>', '<CTRL:dining_table>', '<CTRL:bird>', '<CTRL:animal>', '<CTRL:cat>', '<CTRL:clock>', '<CTRL:cell_phone>', '<CTRL:remote>', '<CTRL:bed>', '<CTRL:microwave>', '<CTRL:oven>', '<CTRL:banana>', '<CTRL:dog>', '<CTRL:couch>', '<CTRL:giraffe>', '<CTRL:cow>', '<CTRL:airplane>', '<CTRL:traffic_light>', '<CTRL:refrigerator>', '<CTRL:orange>', '<CTRL:apple>', '<CTRL:train>', '<CTRL:bicycle>', '<CTRL:zebra>', '<CTRL:kite>', '<CTRL:scissors>', '<CTRL:knife>', '<CTRL:umbrella>', '<CTRL:horse>', '<CTRL:toaster>', '<CTRL:fork>', '<CTRL:sheep>', '<CTRL:teddy_bear>', '<CTRL:bus>', '<CTRL:fire_hydrant>', '<CTRL:sandwich>', '<CTRL:stop_sign>', '<CTRL:suitcase>', '<CTRL:tie>', '<CTRL:toothbrush>', '<CTRL:skateboard>', '<CTRL:parking_meter>', '<CTRL:frisbee>', '<CTRL:skis>', '<CTRL:carrot>', '<CTRL:donut>', '<CTRL:wine_glass>', '<CTRL:surfboard>', '<CTRL:boat>', '<CTRL:hair_drier>', '<CTRL:pizza>', '<CTRL:snowboard>', '<CTRL:broccoli>', '<CTRL:hot_dog>', '<CTRL:elephant>', '<CTRL:baseball_bat>', '<CTRL:baseball_glove>', '<CTRL:bear>', '<CTRL:tennis_racket>']\n",
      "Writing txt\n",
      "txt file already exists, nothing to do here...\n",
      "No tokenizer provided, you will need to provide one later through the tokenize method!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from captions_dataset import CaptionsDataset\n",
    "\n",
    "dataset_train = CaptionsDataset(split=\"train\")\n",
    "dataset_val = CaptionsDataset(split=\"val\")"
   ]
  },
  {
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "print(\"Tokenizer before added special tokens \"+str(len(tokenizer)))\n",
    "special_tokens_dict = {'additional_special_tokens': dataset_train.control_codes}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(\"added \"+str(num_added_toks)+\" tokens to the pretrained tokenizer\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Tokenizer before added special tokens 50259\n",
      "added 91 tokens to the pretrained tokenizer\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Embedding(50350, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0a122091bd17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/conditional_text_generation/src/captions_dataset.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, tokenizer)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mpre_control_codes_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categories'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_train.tokenize(tokenizer)\n",
    "dataset_val.tokenize(tokenizer)"
   ]
  }
 ]
}