{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CtrlUtilities.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO6EFbiGdkSd4Y/yvE0sj/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarinelli/conditional_text_generation/blob/main/notebooks/CtrlUtilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTV-_5bLARf2"
      },
      "source": [
        "experiment_parameters = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5x3qrYk-oI"
      },
      "source": [
        "!pip install --quiet transformers datasets tokenizers sacrebleu wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXm4GhiulVWf"
      },
      "source": [
        "# Dataset\n",
        "We download and load the COCO captions dataset.\n",
        "We join in a single item the caption for an image with the categories and/or supercategories associated to objects present in the image. Categories and/or supercategories are used as control codes depending on the experiment settings.\n",
        "The dataset is then post processed to train the model with different combinations of control codes for each caption, depending on the experiment parameters. The output of the postprocessing is saved on .txt files that are then loaded and further handled by the Dataset class provided by HuggingFace datasets (used for its performance and caching abilities).\n",
        "Here we start with the functions needed to download the COCO captions dataset and preprocess it for our use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6szSXXy_lLdQ"
      },
      "source": [
        "#TODO: Should we move this to an external file? Probably not since it is interesting to show?\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess  # to run sh commands\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from itertools import chain, combinations, groupby\n",
        "\n",
        "!mkdir data\n",
        "DATA_PATH=\"./data\"\n",
        "\n",
        "def download_annotations_dataset(data_path=DATA_PATH):\n",
        "    # download only if don't have it already\n",
        "    if not os.path.isdir(os.path.join(data_path,\"annotations\")):\n",
        "        print(\"Downloading COCO dataset...\")\n",
        "        if not os.path.exists(data_path):\n",
        "            os.makedirs(data_path)\n",
        "        subprocess.run([\"wget\",\"-P\", data_path, \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"])\n",
        "        subprocess.run([\"unzip\", \"-d\", data_path, os.path.join(data_path,\"annotations_trainval2017.zip\")])\n",
        "\n",
        "def map_and_join_dataset(data_instances, data_captions):\n",
        "    if not experiment_parameters[\"use_categories\"] and not experiment_parameters[\"use_supercategories\"]:\n",
        "        print(\"One of categories and supercategories has to be used!\")\n",
        "        sys.exit()\n",
        "\n",
        "    categories_data_dict = dict(map(lambda c: (c[\"id\"], c), data_instances[\"categories\"])) # <category_id, category>\n",
        "    annotations_data_mapped = map(lambda c: (c[\"image_id\"], c), data_instances[\"annotations\"]) # <image_id, annotation>\n",
        "    annotations_data_dict = {}\n",
        "    \n",
        "    for a in annotations_data_mapped:\n",
        "        if a[0] in annotations_data_dict:\n",
        "            annotations_data_dict[a[0]] += [a[1]]\n",
        "        else:\n",
        "            annotations_data_dict[a[0]] = [a[1]]\n",
        "\n",
        "    \n",
        "    captions_data_list = list(map(lambda c: (c[\"image_id\"], c), data_captions[\"annotations\"]))\n",
        "    captions_data_dict = dict()\n",
        "    for image_id, image_captions in groupby(sorted(captions_data_list, key=lambda x: x[0]), lambda x: x[0]): #<image_id, list(caption)>\n",
        "      image_captions_dict = dict()\n",
        "      for caption in image_captions:\n",
        "        image_captions_dict[caption[1][\"id\"]]=caption[1]\n",
        "      captions_data_dict[image_id]=image_captions_dict\n",
        "\n",
        "    dataset = []\n",
        "    control_codes_dict = {}\n",
        "    no_category_counter = 0\n",
        "    references_dict = {}\n",
        "\n",
        "    captions_data_dict_filtered = {}\n",
        "    discarded = 0\n",
        "\n",
        "    \n",
        "\n",
        "    for image_id, captions in captions_data_dict.items():\n",
        "        if len(captions) >= 5:\n",
        "            discarded += max(0, len(captions) - 5)\n",
        "            captions_data_dict_filtered[image_id] = dict(list(captions.items())[0:4])\n",
        "        else:\n",
        "            discarded += len(captions)\n",
        "\n",
        "    print(\"Discarded: \"+str(discarded))\n",
        "\n",
        "    for image_id, captions in captions_data_dict_filtered.items():\n",
        "        #references_dict[image_id] = list(map(lambda x: x[1][\"caption\"], captions.items()))\n",
        "        for _, caption in captions.items():\n",
        "          item = {\"caption\": caption[\"caption\"], \"categories\": [], \"image_id\": image_id}\n",
        "          if image_id in annotations_data_dict:\n",
        "              tmp_categories_dict = {}\n",
        "              for a in annotations_data_dict[image_id]:\n",
        "                  category_name = categories_data_dict[a[\"category_id\"]][\"name\"]\n",
        "                  supercategory_name = categories_data_dict[a[\"category_id\"]][\"supercategory\"]\n",
        "                  if experiment_parameters[\"use_categories\"]:\n",
        "                      tmp_categories_dict[category_name] = 1\n",
        "                      control_codes_dict[category_name] = 1\n",
        "                  if experiment_parameters[\"use_supercategories\"]:\n",
        "                    tmp_categories_dict[supercategory_name] = 1\n",
        "                    control_codes_dict[supercategory_name] = 1\n",
        "              item[\"categories\"]=list(tmp_categories_dict.keys())\n",
        "          if len(item[\"categories\"])==0:\n",
        "              no_category_counter += 1\n",
        "          else: \n",
        "            dataset += [item]\n",
        "            if image_id in references_dict:\n",
        "              references_dict[image_id] += [caption[\"caption\"]]\n",
        "            else:\n",
        "              references_dict[image_id] = [caption[\"caption\"]]\n",
        "\n",
        "\n",
        "    #TODO compute total of captions?\n",
        "\n",
        "    print(\"There are \"+str(no_category_counter)+\" captions without a category\")\n",
        "    return dataset, references_dict, list(control_codes_dict.keys())\n",
        "\n",
        "def load_or_setup_dataset(data_path=DATA_PATH, split='train'):\n",
        "    if not split in ['train', 'val']:\n",
        "        print(\"Unknown split: \"+split)\n",
        "        sys.exit()\n",
        "    if not experiment_parameters[\"force_dataset_update\"] and os.path.isfile(os.path.join(data_path, \"dataset_with_ctrl_\"+split+\".json\")):\n",
        "        print (\"Dataset json file, loading dataset...\")\n",
        "        with open(os.path.join(data_path, \"dataset_with_ctrl_\"+split+\".json\"), \"r\") as read_file:\n",
        "            dataset = json.load(read_file)\n",
        "        with open(os.path.join(data_path, \"control_codes_\"+split+\".json\"), \"r\") as read_file:\n",
        "            control_codes = json.load(read_file)\n",
        "        with open(os.path.join(data_path, \"references_\"+split+\".json\"), \"r\") as read_file:\n",
        "            references_dict = json.load(read_file)\n",
        "    else:\n",
        "        print (\"Dataset json file does not exist, creating dataset from scratch...\")\n",
        "        download_annotations_dataset(data_path=data_path)\n",
        "        with open(os.path.join(data_path,\"annotations/instances_\"+split+\"2017.json\"), \"r\") as read_file:\n",
        "            data_instances = json.load(read_file)\n",
        "\n",
        "        with open(os.path.join(data_path,\"annotations/captions_\"+split+\"2017.json\"), \"r\") as read_file:\n",
        "            data_captions = json.load(read_file)\n",
        "\n",
        "        dataset, references_dict, control_codes = map_and_join_dataset(data_instances, data_captions)\n",
        "\n",
        "        with open(os.path.join(data_path,\"control_codes_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(control_codes, outfile)\n",
        "\n",
        "        with open(os.path.join(data_path,\"references_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(references_dict, outfile)\n",
        "        \n",
        "        with open(os.path.join(data_path,\"dataset_with_ctrl_\"+split+\".json\"), 'w') as outfile:\n",
        "            json.dump(dataset, outfile)\n",
        "    return dataset, references_dict, control_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svCErFRRlaz8"
      },
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "def powerset(iterable, max_size=None):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "    if max_size is None:\n",
        "        return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "    else:\n",
        "        return chain.from_iterable(combinations(s, r) for r in range(min(max_size, len(s)+1)))\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    chunk_number = chunk[0]\n",
        "    chunk_items = chunk[1]\n",
        "    data_path = chunk[2]\n",
        "    split = chunk[3]\n",
        "    json_file = os.path.join(data_path, \"captions_\"+split+\"_\"+str(chunk_number)+\".json\")\n",
        "    captions_array_for_json = []\n",
        "    for item in chunk_items:\n",
        "        if experiment_parameters[\"use_control_codes\"]:\n",
        "            if experiment_parameters[\"use_control_codes_powerset\"]:\n",
        "                control_codes_combinations = powerset(item['categories'], experiment_parameters[\"max_control_codes_per_caption\"])\n",
        "            else:\n",
        "                control_codes_combinations = [item['categories']]\n",
        "        else:\n",
        "            control_codes_combinations = [[]]\n",
        "        for control_codes_combination in control_codes_combinations:\n",
        "            pre_control_codes_string=\"\"\n",
        "            for category in sorted(control_codes_combination):\n",
        "                if experiment_parameters[\"control_codes_type\"] == \"special_token\":\n",
        "                    pre_control_codes_string+=\"<CTRL:\"+category.replace(\" \",\"_\")+\">\"\n",
        "                elif experiment_parameters[\"control_codes_type\"] == \"separators\":\n",
        "                    pre_control_codes_string+=category+\", \"\n",
        "                else:\n",
        "                    print(\"ERROR: wrong control code type\")\n",
        "                    return -1  # TODO here we could fail better\n",
        "            captions_array_for_json += [{\"caption\": pre_control_codes_string+'<|endoftext|>'+item[\"caption\"]+'<|endoftext|>',\"image_id\": item[\"image_id\"]}]\n",
        "    with open(json_file, 'w') as captions_json:\n",
        "        json.dump({\"data\": captions_array_for_json}, captions_json)\n",
        "\n",
        "\n",
        "def write_json_chunks(dataset, split, data_path, chunk_size):\n",
        "    chunks = [dataset[start:min(start+chunk_size,len(dataset))] for start in range(0, len(dataset), chunk_size)]\n",
        "    pool = mp.Pool(processes=8)\n",
        "    pool.map(process_chunk, [(chunk_n, chunk_items, data_path, split) for chunk_n, chunk_items in enumerate(chunks)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erwbsFDBlleV"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcd0nckplj1X"
      },
      "source": [
        "def encode(tokenizer, examples):\n",
        "    encoded = tokenizer(examples['caption'], truncation=True, max_length=64, padding=\"max_length\")\n",
        "    encoded['labels'] = encoded['input_ids']\n",
        "    encoded['image_id'] = examples['image_id']\n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exVyBlBllrpk"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBc-I-cSloCN"
      },
      "source": [
        "import datasets\n",
        "\n",
        "def compute_metrics(tokenizer, references, pred, image_ids):\n",
        "  preds = pred.predictions\n",
        "  metric = datasets.load_metric('sacrebleu')\n",
        "\n",
        "  preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "  references_local_list = [references[image_id.item()] for image_id in image_ids]\n",
        "\n",
        "  final_score = metric.compute(predictions=preds, references=references_local_list)\n",
        "  \n",
        "  return {\n",
        "      'bleu': final_score\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDFJftGl2SF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtn3fuailx5q"
      },
      "source": [
        "#TODO: Should we move this to an external file?\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "import collections\n",
        "import inspect\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "import warnings\n",
        "from logging import StreamHandler\n",
        "from pathlib import Path\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Integrations must be imported before ML frameworks:\n",
        "from transformers.integrations import (  # isort: split\n",
        "    default_hp_search_backend,\n",
        "    get_reporting_integration_callbacks,\n",
        "    hp_params,\n",
        "    is_fairscale_available,\n",
        "    is_optuna_available,\n",
        "    is_ray_tune_available,\n",
        "    run_hp_search_optuna,\n",
        "    run_hp_search_ray,\n",
        "    deepspeed_init,\n",
        "    is_deepspeed_zero3_enabled,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import __version__\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
        "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
        "from transformers.dependency_versions_check import dep_version_check\n",
        "from transformers.file_utils import (\n",
        "    CONFIG_NAME,\n",
        "    WEIGHTS_NAME,\n",
        "    PushToHubMixin,\n",
        "    is_apex_available,\n",
        "    is_datasets_available,\n",
        "    is_in_notebook,\n",
        "    is_sagemaker_dp_enabled,\n",
        "    is_sagemaker_mp_enabled,\n",
        "    is_torch_tpu_available,\n",
        "    is_training_run_on_sagemaker,\n",
        ")\n",
        "from transformers.modelcard import TrainingSummary\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "from transformers.optimization import Adafactor, AdamW, get_scheduler\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.trainer_callback import (\n",
        "    CallbackHandler,\n",
        "    DefaultFlowCallback,\n",
        "    PrinterCallback,\n",
        "    ProgressCallback,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState,\n",
        ")\n",
        "from transformers.trainer_pt_utils import (\n",
        "    DistributedLengthGroupedSampler,\n",
        "    DistributedSamplerWithLoop,\n",
        "    DistributedTensorGatherer,\n",
        "    IterableDatasetShard,\n",
        "    LabelSmoother,\n",
        "    LengthGroupedSampler,\n",
        "    SequentialDistributedSampler,\n",
        "    ShardSampler,\n",
        "    distributed_broadcast_scalars,\n",
        "    distributed_concat,\n",
        "    find_batch_size,\n",
        "    get_parameter_names,\n",
        "    nested_concat,\n",
        "    nested_detach,\n",
        "    nested_numpify,\n",
        "    nested_truncate,\n",
        "    nested_xla_mesh_reduce,\n",
        "    reissue_pt_warnings,\n",
        ")\n",
        "from transformers.trainer_utils import (\n",
        "    PREFIX_CHECKPOINT_DIR,\n",
        "    BestRun,\n",
        "    EvalLoopOutput,\n",
        "    EvalPrediction,\n",
        "    HPSearchBackend,\n",
        "    PredictionOutput,\n",
        "    ShardedDDPOption,\n",
        "    TrainerMemoryTracker,\n",
        "    TrainOutput,\n",
        "    default_compute_objective,\n",
        "    default_hp_space,\n",
        "    denumpify_detensorize,\n",
        "    get_last_checkpoint,\n",
        "    set_seed,\n",
        "    speed_metrics,\n",
        ")\n",
        "from transformers.training_args import ParallelMode, TrainingArguments\n",
        "from transformers.utils import logging\n",
        "from transformers.utils.modeling_auto_mapping import MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES\n",
        "\n",
        "\n",
        "_is_torch_generator_available = False\n",
        "_is_native_amp_available = False\n",
        "\n",
        "DEFAULT_CALLBACKS = [DefaultFlowCallback]\n",
        "DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n",
        "\n",
        "if is_in_notebook():\n",
        "    from transformers.utils.notebook import NotebookProgressCallback\n",
        "\n",
        "    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n",
        "\n",
        "if is_apex_available():\n",
        "    from apex import amp\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
        "    _is_torch_generator_available = True\n",
        "    _is_native_amp_available = True\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "if is_datasets_available():\n",
        "    import datasets\n",
        "\n",
        "if is_torch_tpu_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.debug.metrics as met\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "if is_fairscale_available():\n",
        "    dep_version_check(\"fairscale\")\n",
        "    import fairscale\n",
        "    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n",
        "    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n",
        "    from fairscale.nn.wrap import auto_wrap\n",
        "    from fairscale.optim import OSS\n",
        "    from fairscale.optim.grad_scaler import ShardedGradScaler\n",
        "\n",
        "if is_sagemaker_dp_enabled():\n",
        "    import smdistributed.dataparallel.torch.distributed as dist\n",
        "    from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
        "else:\n",
        "    import torch.distributed as dist\n",
        "\n",
        "if is_sagemaker_mp_enabled():\n",
        "    import smdistributed.modelparallel.torch as smp\n",
        "\n",
        "    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n",
        "\n",
        "if is_training_run_on_sagemaker():\n",
        "    logging.add_handler(StreamHandler(sys.stdout))\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    import optuna\n",
        "\n",
        "logger = logging.get_logger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KxU32IPl9PJ"
      },
      "source": [
        "class MyTrainer(Trainer):\n",
        "    def evaluation_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str,\n",
        "        prediction_loss_only: Optional[bool] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> EvalLoopOutput:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
        "\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        prediction_loss_only = (\n",
        "            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only\n",
        "        )\n",
        "\n",
        "        # if eval is called w/o train init deepspeed here\n",
        "        if self.args.deepspeed and not self.deepspeed:\n",
        "\n",
        "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
        "            # from the checkpoint eventually\n",
        "            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n",
        "            self.model = deepspeed_engine.module\n",
        "            self.model_wrapped = deepspeed_engine\n",
        "            self.deepspeed = deepspeed_engine\n",
        "            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n",
        "            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n",
        "            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n",
        "            deepspeed_engine.optimizer.optimizer = None\n",
        "            deepspeed_engine.lr_scheduler = None\n",
        "\n",
        "        model = self._wrap_model(self.model, training=False)\n",
        "\n",
        "        # if full fp16 is wanted on eval and this ``evaluation`` or ``predict`` isn't called while\n",
        "        # ``train`` is running, halve it first and then put on device\n",
        "        if not self.is_in_train and self.args.fp16_full_eval:\n",
        "            model = model.half().to(self.args.device)\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(f\"***** Running {description} *****\")\n",
        "        if isinstance(dataloader.dataset, collections.abc.Sized):\n",
        "            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n",
        "        else:\n",
        "            logger.info(\"  Num examples: Unknown\")\n",
        "        logger.info(f\"  Batch size = {batch_size}\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        self.callback_handler.eval_dataloader = dataloader\n",
        "        # Do this before wrapping.\n",
        "        eval_dataset = dataloader.dataset\n",
        "\n",
        "        if is_torch_tpu_available():\n",
        "            dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
        "\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = None\n",
        "\n",
        "        # Initialize containers\n",
        "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
        "        losses_host = None\n",
        "        preds_host = None\n",
        "        labels_host = None\n",
        "        # losses/preds/labels on CPU (final containers)\n",
        "        all_losses = None\n",
        "        all_preds = None\n",
        "        all_labels = None\n",
        "        # Will be useful when we have an iterable dataset so don't know its length.\n",
        "\n",
        "        observed_num_examples = 0\n",
        "\n",
        "        #Image ids\n",
        "        all_image_ids = None\n",
        "\n",
        "        # Main evaluation loop\n",
        "        for step, inputs in enumerate(dataloader):\n",
        "            # Update the observed num examples\n",
        "            observed_batch_size = find_batch_size(inputs)\n",
        "            if observed_batch_size is not None:\n",
        "                observed_num_examples += observed_batch_size\n",
        "\n",
        "            # Prediction step\n",
        "            if isinstance(inputs, list):\n",
        "                inputs_for_prediction = [dict(filter(lambda i: i[0]!='image_id', input.items())) for input in inputs]\n",
        "            else:\n",
        "                inputs_for_prediction = dict(filter(lambda i: i[0]!='image_id', inputs.items()))\n",
        "\n",
        "\n",
        "\n",
        "            loss, logits, labels = self.prediction_step(model, inputs_for_prediction, prediction_loss_only, ignore_keys=ignore_keys)\n",
        "\n",
        "            if all_image_ids is None : all_image_ids = []\n",
        "            \n",
        "            if isinstance(inputs, list):\n",
        "                all_image_ids += [input[\"image_id\"] for input in inputs]\n",
        "            else:\n",
        "                all_image_ids += [inputs[\"image_id\"]]\n",
        "\n",
        "\n",
        "            # Update containers on host\n",
        "            if loss is not None:\n",
        "                losses = self._nested_gather(loss.repeat(batch_size))\n",
        "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
        "            ############################\n",
        "            if logits is not None:\n",
        "                logits = self._pad_across_processes(logits)\n",
        "                logits = self._nested_gather(logits)\n",
        "                logits_reduced = np.argmax(logits.cpu(), axis=-1) # Obtain a single value instead of a vector, for memory efficiency\n",
        "                preds_host = logits_reduced if preds_host is None else nested_concat(preds_host, logits_reduced, padding_index=-100)\n",
        "                # preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
        "            ############################\n",
        "            if labels is not None:\n",
        "                labels = self._pad_across_processes(labels)\n",
        "                labels = self._nested_gather(labels)\n",
        "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
        "            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)\n",
        "\n",
        "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
        "            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:\n",
        "                if losses_host is not None:\n",
        "                    losses = nested_numpify(losses_host)\n",
        "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "                if preds_host is not None:\n",
        "                    logits = nested_numpify(preds_host)\n",
        "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "                if labels_host is not None:\n",
        "                    labels = nested_numpify(labels_host)\n",
        "                    all_labels = (\n",
        "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "                    )\n",
        "\n",
        "                # Set back to None to begin a new accumulation\n",
        "                losses_host, preds_host, labels_host = None, None, None\n",
        "\n",
        "        if self.args.past_index and hasattr(self, \"_past\"):\n",
        "            # Clean the state at the end of the evaluation loop\n",
        "            delattr(self, \"_past\")\n",
        "\n",
        "        # Gather all remaining tensors and put them back on the CPU\n",
        "        if losses_host is not None:\n",
        "            losses = nested_numpify(losses_host)\n",
        "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "        if preds_host is not None:\n",
        "            logits = nested_numpify(preds_host)\n",
        "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "        if labels_host is not None:\n",
        "            labels = nested_numpify(labels_host)\n",
        "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "\n",
        "        # Number of samples\n",
        "        if not isinstance(eval_dataset, IterableDataset):\n",
        "            num_samples = len(eval_dataset)\n",
        "        elif isinstance(eval_dataset, IterableDatasetShard):\n",
        "            num_samples = eval_dataset.num_examples\n",
        "        else:\n",
        "            num_samples = observed_num_examples\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses is not None:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds is not None:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels is not None:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "\n",
        "      \n",
        "        # Metrics!\n",
        "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
        "            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels), all_image_ids)\n",
        "        else:\n",
        "            metrics = {}\n",
        "\n",
        "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
        "        metrics = denumpify_detensorize(metrics)\n",
        "\n",
        "        if all_losses is not None:\n",
        "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}