# -*- coding: utf-8 -*-
"""conditional_text_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10jmIPVsLhsdpyxBK_S1igwGOnliPWi1-

# Useful links
## https://huggingface.co/blog/how-to-generate
## https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93 Why it's not working?
## https://huggingface.co/transformers/v2.11.0/model_doc/ctrl.html
## https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93

# Conditional text generation

"""
#%%
# import join used to join ROOT path and GOOGLE_DRIVE_PATH
import os
import subprocess  # to run sh commands
import json

DATA_PATH="../data"

def download_annotations_dataset(data_path="../data"):
  # download only if don't have it already
  if not os.path.isdir(os.path.join(data_path,"annotations")):
    if not os.path.exists(data_path):
      os.makedirs(data_path)
    subprocess.run(["wget","-P", data_path, "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"])
    subprocess.run(["unzip", os.path.join(data_path,"annotations_trainval2017.zip")])

def map_and_join_dataset(data_instances, data_captions):
  categories_data_dict = dict(map(lambda c: (c["id"], c), data_instances["categories"]))
  annotations_data_mapped = map(lambda c: (c["image_id"], c), data_instances["annotations"])
  annotations_data_dict = {}
  for a in annotations_data_mapped:
    if a[0] in annotations_data_dict:
      annotations_data_dict[a[0]] += [a[1]]
    else:
      annotations_data_dict[a[0]] = [a[1]]
  captions_data_dict = dict(map(lambda c: (c["image_id"], c), data_captions["annotations"]))

  dataset = []
  control_codes_dict = {}
  no_category_counter = 0

  for c in captions_data_dict.items():
    item = {"caption": c[1]["caption"], "categories": []}
    if c[1]["image_id"] in annotations_data_dict:
      tmp_categories_dict = {}
      for a in annotations_data_dict[c[1]["image_id"]]:
        category_name = categories_data_dict[a["category_id"]]["name"]
        supercategory_name = categories_data_dict[a["category_id"]]["supercategory"]
        tmp_categories_dict[category_name] = 1
        tmp_categories_dict[supercategory_name] = 1
        control_codes_dict[category_name] = 1
        control_codes_dict[supercategory_name] = 1
      item["categories"]=list(tmp_categories_dict.keys())
      if len(item["categories"])==0:
        no_category_counter += 1
    dataset += [item]

  print("There are "+str(no_category_counter)+" captions without a category")
  return dataset, list(control_codes_dict.keys())

def load_or_setup_dataset(data_path='../data', split='train'):
  if split!='train' and split!='val':
    print("Unknown split: "+split)
    quit(-1)
  if os.path.isfile(os.path.join(data_path, "dataset_with_ctrl_"+split+".json")):
    print ("Dataset json file, loading dataset...")
    with open(os.path.join(data_path, "dataset_with_ctrl_"+split+".json"), "r") as read_file:
      dataset = json.load(read_file)
    with open(os.path.join(data_path, "control_codes_"+split+".json"), "r") as read_file:
      control_codes = json.load(read_file)
  else:
    print ("Dataset json file does not exist, creating dataset from scratch...")
    download_annotations_dataset(data_path=data_path)
    with open(os.path.join(data_path,"annotations/instances_"+split+"2017.json"), "r") as read_file:
      data_instances = json.load(read_file)

    with open(os.path.join(data_path,"annotations/captions_"+split+"2017.json"), "r") as read_file:
      data_captions = json.load(read_file)

    dataset, control_codes = map_and_join_dataset(data_instances, data_captions)

    with open(os.path.join(data_path,"control_codes_"+split+".json"), 'w') as outfile:
      json.dump(control_codes, outfile)
    
    with open(os.path.join(data_path,"dataset_with_ctrl_"+split+".json"), 'w') as outfile:
      json.dump(dataset, outfile)
  return dataset, control_codes

def write_captions_txt(dataset, data_path='../data', split='train'):
  if split!='train' and split!='val':
    print("Unknown split: "+split)
    quit(-1)
  txt_file = os.path.join(data_path, "captions_"+split+".txt")
  if os.path.isfile(txt_file):
    print("txt file already exists, nothing to do here...")
  else:
    with open(txt_file, 'w') as captions_txt:
      for item in dataset:
        pre_control_codes_string=""
        for category in item['categories']:
          pre_control_codes_string+="<CTRL:"+category.replace(" ","_")+">"
        captions_txt.write(pre_control_codes_string+" "+item['caption']+'\n')

dataset_train, control_codes = load_or_setup_dataset(split="train")
dataset_val, _ = load_or_setup_dataset(split="val")

print("There are "+str(len(dataset_train))+" captions in total (training)")
print("There are "+str(len(dataset_val))+" captions in total (validation)")

print("The following "+str(len(control_codes))+" control codes are present in the training dataset:")
print(control_codes)

for i in range(len(control_codes)):
  control_codes[i] = control_codes[i].replace(" ","_")
  control_codes[i] = "<CTRL:"+control_codes[i]+">"

print("Processed control codes:")
print(control_codes)

write_captions_txt(dataset_train, split='train')
write_captions_txt(dataset_val, split='val')

"""# Tokenizer"""

from tokenizers import ByteLevelBPETokenizer
special_tokens=([
      "<s>",
      "<pad>",
      "</s>",
      "<unk>",
      "<mask>",
  ]+control_codes)
if not os.path.isfile(os.path.join(DATA_PATH,"our_ctrl-merges.txt")):
  print("Creating tokenizer from scratch")
  tokenizer = ByteLevelBPETokenizer()
  tokenizer.train(files=[os.path.join(DATA_PATH,"captions_train.txt")], vocab_size=52_000, min_frequency=2, special_tokens=
  special_tokens)

  tokenizer.save_model(DATA_PATH, "our_ctrl")

#from tokenizers.processors import BertProcessing

print("Load tokenizer from files")
tokenizer = ByteLevelBPETokenizer(
    os.path.join(DATA_PATH,"our_ctrl-vocab.json"),
    os.path.join(DATA_PATH,"our_ctrl-merges.txt"),
)
tokenizer.add_special_tokens(special_tokens)
#tokenizer._tokenizer.post_processor = BertProcessing(
#    ("</s>", tokenizer.token_to_id("</s>")),
#    ("<s>", tokenizer.token_to_id("<s>")),
#)
#tokenizer.enable_truncation(max_length=512)

print("Tokenizer example:")
print(
    tokenizer.encode("<CTRL:pizza> A person is walking").ids
)

"""**Dataset**"""

from torch.utils.data import Dataset
import torch
from pathlib import Path

max_length=512

class CaptionDataset(Dataset):
    def __init__(self, evaluate: bool = False):
        tokenizer = ByteLevelBPETokenizer(
            os.path.join(DATA_PATH,"our_ctrl-vocab.json"),
            os.path.join(DATA_PATH,"our_ctrl-merges.txt"),
        )
        tokenizer.add_special_tokens(special_tokens)
        tokenizer.enable_padding()

        self.input_ids = []
        self.attention_masks = []
        self.entries = []

        src_files = Path(DATA_PATH).glob("caption_val.txt") if evaluate else Path(DATA_PATH).glob("captions_train.txt")
        for src_file in src_files:
            print("ðŸ”¥", src_file)
            lines = src_file.read_text(encoding="utf-8").splitlines()
            for x in tokenizer.encode_batch(lines):
                self.input_ids += torch.tensor([x.ids])
                self.attention_masks += torch.tensor([x.attention_mask])
                self.entries += [{"labels": torch.tensor([x.ids]), "input_ids": torch.tensor([x.ids]), "attention_mask": torch.tensor([x.attention_mask])}]

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i):
        # Weâ€™ll pad at the batch level.
        return self.entries[i]


train_dataset = CaptionDataset()

validation_dataset = CaptionDataset(True)

print(train_dataset[0])

"""# Training

**Model instantiation**
"""
#%%
batch_s = 8;
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

train_dataloader = DataLoader(
            train_dataset,  
            batch_size = batch_s
        )

validation_dataloader = DataLoader(
            validation_dataset, 
            batch_size = batch_s
        )

import random
import torch
from transformers import GPT2LMHeadModel, GPT2Config
import numpy as np

# Loading the model configuration and setting it to the GPT2 standard settings.
configuration = GPT2Config.from_pretrained('distilgpt2', output_hidden_states=False)

# Create the instance of the model and set the token size embedding length
model = GPT2LMHeadModel.from_pretrained("distilgpt2", config=configuration)
model.resize_token_embeddings(19520)

# Tell pytorch to run this model on the GPU.
device = torch.device("cuda")
model.cuda()

# This step is optional but will enable reproducible runs.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

"""**Training parameters**"""

# We wil create a few variables to define the training parameters of the model
# epochs are the training rounds
# the warmup steps are steps at the start of training that are ignored
# every x steps we will sample the model to test the output

epochs = 5
warmup_steps = 1e2
sample_every = 1000

from transformers import AdamW
# AdamW is a class from the huggingface library, it is the optimizer we will be using, and we will only be instantiating it with the default parameters. 
optimizer = AdamW(model.parameters(),
                  lr = 5e-4,
                  eps = 1e-8
                )

from transformers import get_linear_schedule_with_warmup

total_steps = len(validation_dataloader) * epochs

"""
We can set a variable learning rate which will help scan larger areas of the 
problem space at higher LR earlier, then fine tune to find the exact model minima 
at lower LR later in training.
"""
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = warmup_steps, 
                                            num_training_steps = total_steps)

sequence = 'A table'
input_ids  = tokenizer.encode(sequence)
input_ids

from transformers import Trainer, TrainingArguments


#model = model(input_ids)
training_args = TrainingArguments(
    output_dir="./results",          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=96,  # batch size per device during training
    per_device_eval_batch_size=96,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=validation_dataset)

# https://huggingface.co/transformers/glossary.html#input-ids
# https://huggingface.co/transformers/training.html
#sequence = "A table"
#encoded = tokenizer.encode(sequence)
#input_ids= encoded.ids
trainer.train()

import random
import time
import datetime
import torch

def format_time(elapsed):
    return str(datetime.timedelta(seconds=int(round((elapsed)))))

total_t0 = time.time()

training_stats = []

model = model.to(device)

for epoch_i in range(0, epochs):

    print(f'Beginning epoch {epoch_i + 1} of {epochs}')

    t0 = time.time()

    total_train_loss = 0

    model.train()

    for step, batch in enumerate(train_dataloader):

        b_input_ids = batch["input_ids"].to(device)
        b_labels = batch["labels"].to(device)
        b_masks = batch["attention_mask"].to(device)

        model.zero_grad()        

        outputs = model(  b_input_ids,
                          labels=b_labels, 
                          attention_mask = b_masks,
                          token_type_ids=None
                        )

        loss = outputs[0]  

        batch_loss = loss.item()
        total_train_loss += batch_loss

        # Get sample every 100 batches.
        if step % sample_every == 0 and not step == 0:

            elapsed = format_time(time.time() - t0)
            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')

            model.eval()
            test_input=torch.ones(1,2, dtype=torch.int).to(device)
            attention_mask=torch.zeros(1,2, dtype=torch.long).to(device)
            test_input[0][0]=0
            test_input[0][1]=random.randint(5, 19520)
            attention_mask[0][0]=1
            attention_mask[0][1]=1
            sample_outputs = model.generate(
                                    input_ids=test_input,
                                    attention_mask=attention_mask,
                                    bos_token_id=0,
                                    do_sample=True,   
                                    top_k=50, 
                                    max_length = 512,
                                    top_p=0.95, 
                                    num_return_sequences=1,
                                    eos_token_id=2,
                                    pad_token_id=1
                                )
            for i, sample_output in enumerate(sample_outputs):
                  print(f'Test input: {tokenizer.decode(test_input[0].tolist(), skip_special_tokens=True)}')
                  print(f'Example output: {tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)}')

            model.train()

        loss.backward()

        optimizer.step()

        scheduler.step()

    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)       
    
    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')

    t0 = time.time()

    model.eval()

    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        
        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device) #TODO: controllare come funziona valutazione con mask!
        
        with torch.no_grad():        

            outputs  = model(b_input_ids,  
                             attention_mask = b_masks,
                             labels=b_labels)
          
            loss = outputs[0]  
            
        batch_loss = loss.item()
        total_eval_loss += batch_loss        

    avg_val_loss = total_eval_loss / len(validation_dataloader)
    
    validation_time = format_time(time.time() - t0)    

    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print(f'Total training took {format_time(time.time()-total_t0)}')

from transformers import PreTrainedModel

# If we are executing this function, we are the process zero, so we don't check for that.
output_dir = join(PROJECT_PATH,"output")
os.makedirs(output_dir, exist_ok=True)
# Save a trained model and configuration using `save_pretrained()`.
# They can then be reloaded using `from_pretrained()`
if not isinstance(model, PreTrainedModel):
    if isinstance(unwrap_model(model), PreTrainedModel):
        if state_dict is None:
            state_dict = model.state_dict()
        unwrap_model(model).save_pretrained(output_dir, state_dict=state_dict)
    else:
        logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
        if state_dict is None:
            state_dict = model.state_dict()
        torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
else:
    model.save_pretrained(output_dir, state_dict=None)
#if tokenizer is not None:
#    tokenizer.save_pretrained(output_dir)
# %%
